{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a neural network with PyTorch - Study of the Breast Cancer Wisconsin Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to get familiar with both virtual environments and PyTorch. We also want to compare NN network performances with the one we built - [Building a Neural Network](#https://github.com/Gdeterline/Neural-Network-Build/blob/main/breast_cancer_classification.ipynb), using only numpy. We evaluated the model performance on the Breast Cancer Wisconsin dataset.\n",
    "Developing a neural network with PyTorch - Study of the Breast Cancer Wisconsin Dataset is therefore a way to hit two birds with one stone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from PyTorchModel import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = pd.read_csv('./datasets/breast_cancer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess the data the same way we did in the previous notebook.\n",
    "This time, we will use PyTorch to build the neural network.\n",
    "\n",
    "<ins>Nota Bene:</ins> It is important to note that the absolute performance of the model is not the main focus. In the previous project, we chose to focus on several features of the input data. We will focus on the same ones here. Therefore, this could lead to a lower performance of the model. But at least, we'll be able to compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> \n",
      " <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\n",
    "    'diagnosis', \n",
    "    'radius_mean', \n",
    "    'perimeter_mean', \n",
    "    'area_mean', \n",
    "    'concavity_mean', \n",
    "    'concave points_mean', \n",
    "    'radius_se', \n",
    "    'perimeter_se', \n",
    "    'area_se', \n",
    "    'radius_worst', \n",
    "    'perimeter_worst', \n",
    "    'area_worst', \n",
    "    'compactness_worst', \n",
    "    'concavity_worst', \n",
    "    'concave points_worst'\n",
    "]\n",
    "\n",
    "# Create the new DataFrame with the selected columns\n",
    "data_postpr = data[selected_columns].copy(deep=True)\n",
    "\n",
    "# Split X and y data\n",
    "y = data_postpr['diagnosis']\n",
    "X = data_postpr.drop('diagnosis', axis=1)\n",
    "\n",
    "print(type(X), '\\n', type(y))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'> \n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), '\\n', type(X_test), '\\n', type(y_train), '\\n', type(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now loaded and preprocessed exactly as in the previous notebook. We will now build the neural network using PyTorch. \n",
    "\n",
    "First, we need to convert the arrays to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_train = y_train.unsqueeze(1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([455, 14])\n",
      "torch.Size([455, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready to be used in the neural network. We will now build the neural network, train it, and evaluate its performance.\n",
    "\n",
    "We will apply the following steps:\n",
    "- Define the neural network\n",
    "- Define the loss function\n",
    "- Define the optimizer\n",
    "- Train the network\n",
    "- Evaluate the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.6624242663383484\n",
      "Epoch 500 - Loss: 0.6580744981765747\n",
      "Epoch 1000 - Loss: 0.654190182685852\n",
      "Epoch 1500 - Loss: 0.6502673625946045\n",
      "Epoch 2000 - Loss: 0.6462366580963135\n",
      "Epoch 2500 - Loss: 0.6420353651046753\n",
      "Epoch 3000 - Loss: 0.6376010775566101\n",
      "Epoch 3500 - Loss: 0.6328703165054321\n",
      "Epoch 4000 - Loss: 0.6277782917022705\n",
      "Epoch 4500 - Loss: 0.6222587823867798\n",
      "Epoch 5000 - Loss: 0.6162433624267578\n",
      "Epoch 5500 - Loss: 0.6096624732017517\n",
      "Epoch 6000 - Loss: 0.6024459004402161\n",
      "Epoch 6500 - Loss: 0.5945242643356323\n",
      "Epoch 7000 - Loss: 0.5858305096626282\n",
      "Epoch 7500 - Loss: 0.576303243637085\n",
      "Epoch 8000 - Loss: 0.5658897757530212\n",
      "Epoch 8500 - Loss: 0.55455082654953\n",
      "Epoch 9000 - Loss: 0.5422652959823608\n",
      "Epoch 9500 - Loss: 0.5290355086326599\n",
      "Epoch 10000 - Loss: 0.5148926377296448\n",
      "Epoch 10500 - Loss: 0.4999004304409027\n",
      "Epoch 11000 - Loss: 0.4841579496860504\n",
      "Epoch 11500 - Loss: 0.46779853105545044\n",
      "Epoch 12000 - Loss: 0.4509868323802948\n",
      "Epoch 12500 - Loss: 0.43391114473342896\n",
      "Epoch 13000 - Loss: 0.41677325963974\n",
      "Epoch 13500 - Loss: 0.3997770845890045\n",
      "Epoch 14000 - Loss: 0.38311633467674255\n",
      "Epoch 14500 - Loss: 0.36696404218673706\n",
      "Epoch 15000 - Loss: 0.3514643609523773\n",
      "Epoch 15500 - Loss: 0.33672913908958435\n",
      "Epoch 16000 - Loss: 0.32283589243888855\n",
      "Epoch 16500 - Loss: 0.3098298907279968\n",
      "Epoch 17000 - Loss: 0.29772794246673584\n",
      "Epoch 17500 - Loss: 0.2865234613418579\n",
      "Epoch 18000 - Loss: 0.2761916518211365\n",
      "Epoch 18500 - Loss: 0.2666943371295929\n",
      "Epoch 19000 - Loss: 0.2579846978187561\n",
      "Epoch 19500 - Loss: 0.2500108480453491\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001) # Stochastic Gradient Descent - same LR as use case NN built\n",
    "loss_func = nn.BCELoss()\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(X_train)\n",
    "    loss = loss_func(y_hat, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained. We will now evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training dataset : 0.9296703338623047%\n",
      "Accuracy on the testing dataset : 0.9561403393745422%\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model(X_train)\n",
    "accuracy_train = (y_pred_train.round() == y_train).float().mean()\n",
    "print(f'Accuracy on the training dataset : {accuracy_train}%')\n",
    "\n",
    "y_pred_test = model(X_test)\n",
    "accuracy_test = (y_pred_test.round() == y_test).float().mean()\n",
    "print(f'Accuracy on the testing dataset : {accuracy_test}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the exact same conditions, the neural network we built ourselves works better than the one we built with PyTorch.\n",
    "Yet there are two main things we need to improve in the model we built ourselves:\n",
    "- The computation time is much longer (still pretty short for this small dataset). Ours takes about 1min30 to train, while the PyTorch model takes about 30s.\n",
    "- The PyTorch model is much more flexible - the activation functions can change easily depending on the layer, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
